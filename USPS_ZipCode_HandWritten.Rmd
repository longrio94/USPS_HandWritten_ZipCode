---
title: "Digit Recognizer"
author: "Hai Long, Le- 18200524"
date: "July 6, 2019"
output: html_document
---


#### Data on handwritten zip-code digits from the United States Postal Serviceis  divided  into  the  data  sets  train  (7,291  observations)  and  `test'  (2,007observations). Of the 257 columnsin  both  datasets,  the   rst  contains  the  known  data  labels,  which  simplyrepresent  the  identity  of  each  handwritten  image  [0,..,9].   The  remaining 256  columns,  for  each  single  digit,  represent  the  concatenation  of  a  16x16greyscale  map  matrix  into  a  vector  of  length  256,  using  the  scale  [-1,1],corresponding to [black, white].  This matrix is obtained by subdividing thedigit entry box into a 16x16 grid and then scanning.



#### I aim to practice the Tuning Parameters with different Kernels of SVM utilizing Caret package. I also use MLP Neural Network with Keras and CNN using Keras to compare the accuracy with Parameterized SVM. 



```{r message=FALSE, warning=FALSE}
library(keras)
library(caret)
library(MLmetrics)
library(kernlab)
```


# 1. Import Data

```{r}
train <- read.table("train.txt", header = F)
test <- read.table("test.txt", header = F)
```


```{r}
dim(train)
```


```{r message=FALSE, warning=F}
library(doParallel)
cl <- makeCluster(2)
registerDoParallel(cl)
```



# 2. Model Building


* The dataset is already scaled to [-1,1] range corresponding to [white,black].
* PreProcess using PCA. I want to build a "segmenter" using PCA first and then build the "classifier" later.

```{r message=FALSE, warning=F}
preProcValues <- preProcess(train[,-1], method = "pca")
# transformed for Train
transformed_train <- predict(preProcValues, train[,-1])
transformed_train$response <- train[,1]
# convert Response to Catogorical / Rounded Numbers from [0,....9]
transformed_train$response <- as.factor(transformed_train$response)
# transformed for Test
transformed_test <- predict(preProcValues, test[,-1])
transformed_test$response <- test[,1]
```



## 2.1  "Polynomial" Kernel with Tuning Parameters

* Tuning Parameters on C, Degree, scale=1.


```{r}
tune.poly <- expand.grid(C= c(0.1,1,10,100), degree=c(1,2,3) ,scale= 1)
my_Control <- trainControl(method="cv",number=5)
set.seed(123)
sel.poly <- train(response ~ . , data= transformed_train,method="svmPoly",trControl=my_Control,tuneGrid=tune.poly)
caret_poly_predict <- predict(sel.poly, transformed_test[,-257])
```


#### Accuracy with Test Set.

```{r}
mean(caret_poly_predict == transformed_test$response)
```

#### Model Summary

```{r}
sel.poly
```



## 2.2 "Radial" Kernel with Tuning Parameters

* Tuning Parameters on C, Sigma.

```{r}
tune.radial <- expand.grid(C= c(0.01, 0.1, 1, 5, 10), sigma = c(0.001, 0.01, 0.1, 1, 5)) 
my_Control <- trainControl(method="cv",number=5)
set.seed(123)
sel.radial <- train(response ~ . , data= transformed_train,method="svmRadial",trControl=my_Control,tuneGrid=tune.radial)
caret_radial_predict <- predict(sel.radial, transformed_test[,-257])
```


#### Accuracy with Test Set.
```{r}
mean(caret_radial_predict == transformed_test$response)
```


#### Model Summary after Tuning Parameters
```{r}
sel.radial
```



## 2.3 Deep Learning MLP

```{r}
train <- read.table("train.txt", header = F)
test <- read.table("test.txt", header = F)
```

```{r}
x_train <- train[,2:257]
y_train <- train[,1]
x_test <- test[,2:257]
y_test <- test[,1]
```


```{r}
y_train <- as.matrix(y_train)
y_test <- as.matrix(y_test)
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
x_train <- as.matrix(x_train)
y_train <- as.matrix(y_train)
```



#### Step1: Defining the Model


```{r}
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 256, activation = 'relu', input_shape = c(256)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dropout(rate = 0.25) %>%
  layer_dense(units = 10, activation = 'softmax')
```


#### Step 2: Compile the Model


```{r}
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = 'adam',
metrics = c('accuracy')
)
```


#### Step 3: Fit to Training Dataset

```{r}
history <- fit(
  object           = model, 
  x                = as.matrix(x_train), 
  y                = y_train,
  batch_size       = 50,
  epochs           = 35,
  validation_split = 0.3
)
```


```{r}
print(history)
```


```{r}
yhat_keras_class_vec <- predict_classes(object = model, x = as.matrix(x_test)) %>%
    as.vector()
```


#### Accuracy of MLP

```{r}
mean(yhat_keras_class_vec == test[,1])
```



## 2.4 CNN (Convolutional neural network)

```{r}
train <- read.table("train.txt", header = F)
test <- read.table("test.txt", header = F)
```

```{r}
train<-data.matrix(train)
test<-data.matrix(test)
```


```{r}
x_train <- train[,2:257]
y_train <- train[,1]
x_test <- test[,2:257]
y_test <- test[,1]
```


```{r}
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
```


```{r}
## convert to 4d array
x_train_cnn<-array_reshape(data.matrix(train[,2:257]),c(nrow(train),16,16,1))
```

```{r}
#Data partition
y_train_cnn<-data.matrix(train[,1])
y_train_cnn<-to_categorical(y_train_cnn)
```



```{r}
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu",
                input_shape = c(16, 16, 1)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dropout(rate=0.5) %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 10, activation = "softmax")
```



```{r}
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = 'adam',
metrics = c('accuracy')
)
```



```{r}
history<- model %>% fit(
    x_train_cnn,y_train_cnn,
    epochs=10,
    batch_size=32)
```



```{r}
history
```



```{r}
x_test_cnn <-array_reshape(data.matrix(test[,2:257]),c(nrow(test),16,16,1))
y_test_cnn<-data.matrix(train[,1])
```


```{r}
yhat_keras_class_vec <- predict_classes(object = model, x = x_test_cnn) %>%
    as.vector()
```



#### Accuracy of CNN

```{r}
mean(yhat_keras_class_vec == testing[,1])
```
